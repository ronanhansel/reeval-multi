{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a0296cc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "import json\n",
        "import gc\n",
        "from torch.distributions import Bernoulli\n",
        "from torch.optim import LBFGS\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import pearsonr\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from multiprocessing import Manager\n",
        "import multiprocessing as mp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from tueplots import bundles\n",
        "bundles.icml2024()\n",
        "\n",
        "from torchmetrics import AUROC\n",
        "auroc = AUROC(task=\"binary\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "def visualize_response_matrix(results, value, filename):\n",
        "    # Extract the groups labels in the order of the columns\n",
        "    group_values = results.columns.get_level_values(\"scenario\")\n",
        "\n",
        "    # Identify the boundaries where the group changes\n",
        "    boundaries = []\n",
        "    for i in range(1, len(group_values)):\n",
        "        if group_values[i] != group_values[i - 1]:\n",
        "            boundaries.append(i - 0.5)  # using 0.5 to place the line between columns\n",
        "\n",
        "    # Visualize the results with a matrix: red is 0, white is -1 and blue is 1\n",
        "    cmap = mcolors.ListedColormap([\"white\", \"red\", \"blue\"])\n",
        "    bounds = [-1.5, -0.5, 0.5, 1.5]\n",
        "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    # Calculate midpoints for each group label\n",
        "    groups_list = list(group_values)\n",
        "    group_names = []\n",
        "    group_midpoints = []\n",
        "    current_group = groups_list[0]\n",
        "    start_index = 0\n",
        "    for i, grp in enumerate(groups_list):\n",
        "        if grp != current_group:\n",
        "            midpoint = (start_index + i - 1) / 2.0\n",
        "            group_names.append(current_group)\n",
        "            group_midpoints.append(midpoint)\n",
        "            current_group = grp\n",
        "            start_index = i\n",
        "    # Add the last group\n",
        "    midpoint = (start_index + len(groups_list) - 1) / 2.0\n",
        "    group_names.append(current_group)\n",
        "    group_midpoints.append(midpoint)\n",
        "\n",
        "    # Define the minimum spacing between labels (e.g., 100 units)\n",
        "    min_spacing = 100\n",
        "    last_label_pos = -float(\"inf\")\n",
        "    # Plot the matrix\n",
        "    with plt.rc_context(bundles.icml2024(usetex=True, family=\"serif\")):\n",
        "        fig, ax = plt.subplots(figsize=(20, 10))\n",
        "        cax = ax.matshow(value, aspect=\"auto\", cmap=cmap, norm=norm)\n",
        "\n",
        "        # Add vertical lines at each boundary\n",
        "        for b in boundaries:\n",
        "            ax.axvline(x=b, color=\"black\", linewidth=0.25, linestyle=\"--\", alpha=0.5)\n",
        "        \n",
        "        # Add group labels above the matrix, only if they're spaced enough apart\n",
        "        for name, pos in zip(group_names, group_midpoints):\n",
        "            if pos - last_label_pos >= min_spacing:\n",
        "                ax.text(pos, -5, name, ha='center', va='bottom', rotation=90, fontsize=3)\n",
        "                last_label_pos = pos\n",
        "\n",
        "        # Add model labels on the y-axis\n",
        "        ax.set_yticks(range(len(results.index)))\n",
        "        ax.set_yticklabels(results.index, fontsize=3)\n",
        "\n",
        "        # Add a colorbar\n",
        "        cbar = plt.colorbar(cax)\n",
        "        cbar.set_ticks([-1, 0, 1])\n",
        "        cbar.set_ticklabels([\"-1\", \"0\", \"1\"])\n",
        "        plt.savefig(filename, dpi=600, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "\n",
        "def trainer(parameters, optim, closure, n_iter=100, verbose=True):\n",
        "    pbar = tqdm(range(n_iter)) if verbose else range(n_iter)\n",
        "    for iteration in pbar:\n",
        "        if iteration > 0:\n",
        "            previous_parameters = [p.clone() for p in parameters]\n",
        "            previous_loss = loss.clone()\n",
        "        \n",
        "        loss = optim.step(closure)\n",
        "        \n",
        "        if iteration > 0:\n",
        "            d_loss = (previous_loss - loss).item()\n",
        "            d_parameters = sum(\n",
        "                torch.norm(prev - curr, p=2).item()\n",
        "                for prev, curr in zip(previous_parameters, parameters)\n",
        "            )\n",
        "            grad_norm = sum(torch.norm(p.grad, p=2).item() for p in parameters if p.grad is not None)\n",
        "            if verbose:\n",
        "                pbar.set_postfix({\"grad_norm\": grad_norm, \"d_parameter\": d_parameters, \"d_loss\": d_loss})\n",
        "            \n",
        "            if d_loss < 1e-5 and d_parameters < 1e-5 and grad_norm < 1e-5:\n",
        "                break\n",
        "    return parameters\n",
        "\n",
        "def compute_auc(probs, data, train_idtor, test_idtor):\n",
        "    train_probs = probs[train_idtor.bool()]\n",
        "    test_probs = probs[test_idtor.bool()]\n",
        "    train_labels = data[train_idtor.bool()]\n",
        "    test_labels = data[test_idtor.bool()]\n",
        "    train_auc = auroc(train_probs, train_labels)\n",
        "    test_auc = auroc(test_probs, test_labels)\n",
        "    print(f\"train auc: {train_auc}\")\n",
        "    print(f\"test auc: {test_auc}\")\n",
        "    \n",
        "    return train_auc, test_auc\n",
        "\n",
        "def compute_cttcorr(probs, data, train_idtor, test_idtor):\n",
        "    train_probs  = probs.clone()\n",
        "    test_probs   = probs.clone()\n",
        "    train_labels = data.clone()\n",
        "    test_labels  = data.clone()\n",
        "\n",
        "    train_mask = ~train_idtor.bool()\n",
        "    train_probs[train_mask]  = float('nan')\n",
        "    train_labels[train_mask] = float('nan')\n",
        "\n",
        "    test_mask = ~test_idtor.bool()\n",
        "    test_probs[test_mask]   = float('nan')\n",
        "    test_labels[test_mask]  = float('nan')\n",
        "    \n",
        "    train_prob_ctt = torch.nanmean(train_probs, dim=1).detach().cpu().numpy()\n",
        "    train_label_ctt = torch.nanmean(train_labels, dim=1).detach().cpu().numpy()\n",
        "    train_mask = ~np.isnan(train_prob_ctt) & ~np.isnan(train_label_ctt)\n",
        "    train_cttcorr = pearsonr(train_prob_ctt[train_mask], train_label_ctt[train_mask]).statistic\n",
        "    \n",
        "    test_prob_ctt = torch.nanmean(test_probs, dim=1).detach().cpu().numpy()\n",
        "    test_label_ctt = torch.nanmean(test_labels, dim=1).detach().cpu().numpy()\n",
        "    test_mask = ~np.isnan(test_prob_ctt) & ~np.isnan(test_label_ctt)\n",
        "    test_cttcorr = pearsonr(test_prob_ctt[test_mask], test_label_ctt[test_mask]).statistic\n",
        "    \n",
        "    print(f\"train cttcorr: {train_cttcorr}\")\n",
        "    print(f\"test cttcorr: {test_cttcorr}\")\n",
        "\n",
        "    return train_cttcorr, test_cttcorr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "13e77c09",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import float32\n",
        "\n",
        "\n",
        "with open(f\"../data/resmat.pkl\", \"rb\") as f:\n",
        "    results = pickle.load(f)\n",
        "\n",
        "dtype = torch.float64 if device.startswith(\"cuda\") else torch.float32\n",
        "\n",
        "# data_withnan, missing=nan\n",
        "# data_withneg1, missing=-1\n",
        "# data_with0, missing=0\n",
        "data_withnan = torch.tensor(results.values, dtype=dtype, device=device)\n",
        "data_idtor = (~torch.isnan(data_withnan)).to(dtype)\n",
        "data_withneg1 = data_withnan.nan_to_num(nan=-1.0)\n",
        "data_with0 = data_withneg1 * data_idtor\n",
        "data_with0 = data_with0.nan_to_num(nan=0.0)\n",
        "n_test_takers, n_items = data_with0.shape\n",
        "scenarios = results.columns.get_level_values(\"scenario\").unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "47d3fa9a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1.], device='cuda:0', dtype=torch.float64)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_with0.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d48933b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expanding Q-Matrix to question level...\n",
            "Shape of expanded Q-Matrix: torch.Size([78712, 3])\n",
            "trial 0 valid condition: True\n",
            "Starting Stage 1: Fitting item parameters (a_params and ds)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stage 1: Calibrating Item Batches:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stage 1: Calibrating Item Batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.69s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stage 1 Finished.\n",
            "Starting Stage 2: Fitting person parameters (thetas)...\n",
            "Stage 2 Finished. Training complete.\n",
            "train auc: 0.8100907802581787\n",
            "test auc: 0.7944523096084595\n",
            "train cttcorr: 0.703482087615883\n",
            "test cttcorr: 0.6817904579591438\n",
            "\n",
            "--- Post-Hoc Analysis ---\n",
            "Theta Correlation Matrix:\n",
            "[[1.         0.10632631 0.0273906 ]\n",
            " [0.10632631 1.         0.23484477]\n",
            " [0.0273906  0.23484477 1.        ]]\n",
            "\n",
            "Interpretation:\n",
            "- Values close to 0 mean the dimensions are distinct.\n",
            "- Values close to 1 or -1 mean the dimensions are highly related.\n",
            "\n",
            "--- Model Fit Indices ---\n",
            "Number of parameters (k): 315397\n",
            "Number of observations (n): 4267308\n",
            "\n",
            "Total Log-Likelihood: -2215706.13\n",
            "AIC: 5062206.25\n",
            "BIC: 9246418.75\n",
            "\n",
            "Reminder: Lower AIC/BIC values indicate a better model fit.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.distributions import Bernoulli\n",
        "from torch.optim import LBFGS\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "import gc\n",
        "\n",
        "# Assume prerequisite functions like trainer() and data tensors are loaded\n",
        "# For this example, we will simulate the necessary data structures.\n",
        "\n",
        "# ====================================================================================\n",
        "# 1. SETUP & DATA SIMULATION (Replace with your actual data loading)\n",
        "# ====================================================================================\n",
        "\n",
        "n_test_takers = 183\n",
        "n_items = 78712  # <-- FIX 1: Set n_items to the total number of questions\n",
        "n_dimensions = 3\n",
        "\n",
        "# --- Create a map from each question to its scenario ---\n",
        "# CRITICAL STEP: You need to generate this map from your data.\n",
        "# We will simulate it here. Assume you have a DataFrame or list that\n",
        "# tells you which scenario each of the 78,712 questions belongs to.\n",
        "# For example, the first 5236 questions are from 'air_bench_2024' (scenario index 0),\n",
        "# the next 9558 are from 'babi_qa' (scenario index 1), etc.\n",
        "\n",
        "question_to_scenario_map = np.fromfile(\"../data/scenario_map.npy\", dtype=np.int32)\n",
        "\n",
        "\n",
        "# ====================================================================================\n",
        "# 2. EXPAND THE Q-MATRIX\n",
        "# ====================================================================================\n",
        "\n",
        "# Your original (22, 3) Q-Matrix for scenarios\n",
        "q_matrix_scenario = np.array([\n",
        "    [1, 0, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 1, 0],\n",
        "    [1, 0, 0], [1, 1, 0], [1, 0, 0], [1, 0, 1], [1, 0, 0],\n",
        "    [1, 0, 1], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 1],\n",
        "    [1, 0, 0], [1, 1, 0], [1, 1, 1], [1, 0, 0], [1, 0, 0],\n",
        "    [1, 0, 0], [1, 0, 0]\n",
        "])\n",
        "\n",
        "# FIX 2: Expand the Q-Matrix to the question level using the map\n",
        "print(\"Expanding Q-Matrix to question level...\")\n",
        "q_matrix_np_expanded = q_matrix_scenario[question_to_scenario_map]\n",
        "Q_matrix = torch.tensor(q_matrix_np_expanded, device=device, dtype=torch.float32)\n",
        "\n",
        "print(f\"Shape of expanded Q-Matrix: {Q_matrix.shape}\") # Should be (78712, 3)\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.distributions import Bernoulli\n",
        "from torch.optim import LBFGS\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "# Assume prerequisite data tensors are loaded...\n",
        "# device, data_with0, data_idtor, Q_matrix, n_test_takers, n_items, n_dimensions\n",
        "\n",
        "# ====================================================================================\n",
        "# 3. MIRT MODEL TRAINING (Corrected for LBFGS)\n",
        "# ====================================================================================\n",
        "\n",
        "# Apply random train/test mask to the matrix\n",
        "valid_condition = False\n",
        "trial = 0\n",
        "while not valid_condition:\n",
        "    train_idtor = torch.bernoulli(data_idtor * 0.8).int()\n",
        "    test_idtor = data_idtor - train_idtor\n",
        "    valid_condition = (train_idtor.sum(axis=1) != 0).all() and (train_idtor.sum(axis=0) != 0).all()\n",
        "    print(f\"trial {trial} valid condition: {valid_condition}\")\n",
        "    trial += 1\n",
        "\n",
        "# --- STAGE 1: Fit Item Parameters (a_params and ds) ---\n",
        "print(\"Starting Stage 1: Fitting item parameters (a_params and ds)...\")\n",
        "\n",
        "# Use a fixed, random set of thetas to approximate the expectation (Monte Carlo EM)\n",
        "n_mc_samples = 150\n",
        "thetas_nuisance = torch.randn(n_mc_samples, n_test_takers, n_dimensions, device=device)\n",
        "\n",
        "# Initialize parameters\n",
        "a_params = torch.randn(n_items, n_dimensions, requires_grad=True, device=device)\n",
        "ds = torch.randn(n_items, requires_grad=True, device=device)\n",
        "\n",
        "# Process items in batches to make LBFGS memory-efficient\n",
        "B = 50000\n",
        "for i in tqdm(range(0, n_items, B), desc=\"Stage 1: Calibrating Item Batches\"):\n",
        "    \n",
        "    # <<< FIX: Create new, independent LEAF tensors for each batch\n",
        "    # We clone the data from the main tensor, detach it from the graph,\n",
        "    # and then set requires_grad=True to make it a new leaf.\n",
        "    current_B = min(B, n_items - i)\n",
        "    a_params_batch = a_params[i:i+current_B].clone().detach().requires_grad_(True)\n",
        "    ds_batch = ds[i:i+current_B].clone().detach().requires_grad_(True)\n",
        "\n",
        "    # Select corresponding data and Q-matrix for the batch\n",
        "    data_batch = data_with0[:, i:i+current_B]\n",
        "    train_idtor_batch = train_idtor[:, i:i+current_B]\n",
        "    Q_matrix_batch = Q_matrix[i:i+current_B, :]\n",
        "\n",
        "    # Define the optimizer for the current batch (now with valid leaf tensors)\n",
        "    optim_items = LBFGS([a_params_batch, ds_batch], lr=0.1, max_iter=20, history_size=10, line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "    # Define the closure function required by LBFGS\n",
        "    def closure_items():\n",
        "        optim_items.zero_grad()\n",
        "        \n",
        "        a_params_masked = a_params_batch * Q_matrix_batch\n",
        "        a_params_constrained = torch.clamp(a_params_masked, min=0)\n",
        "        \n",
        "        logits = torch.matmul(thetas_nuisance, a_params_constrained.T) - ds_batch[None, None, :]\n",
        "        probs = torch.sigmoid(logits)\n",
        "        \n",
        "        log_likelihoods = Bernoulli(probs=probs).log_prob(data_batch[None, :, :]) * train_idtor_batch[None, :, :]\n",
        "        loss = -log_likelihoods.sum() / (train_idtor_batch.sum() * n_mc_samples)\n",
        "        \n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    # The step function now calls the closure\n",
        "    optim_items.step(closure_items)\n",
        "    \n",
        "    # <<< FIX: Copy the optimized data from the batch tensor back to the main tensor\n",
        "    # We use .data to do this without affecting the gradient history.\n",
        "    a_params.data[i:i+current_B] = a_params_batch.data\n",
        "    ds.data[i:i+current_B] = ds_batch.data\n",
        "\n",
        "\n",
        "# Detach the now-calibrated item parameters to fix them for the next stage\n",
        "a_params_calibrated = a_params.detach()\n",
        "ds_calibrated = ds.detach()\n",
        "print(\"Stage 1 Finished.\")\n",
        "\n",
        "\n",
        "# --- STAGE 2: Fit Person Parameters (thetas) ---\n",
        "print(\"Starting Stage 2: Fitting person parameters (thetas)...\")\n",
        "\n",
        "# Initialize the real thetas we want to learn\n",
        "thetas = torch.randn(n_test_takers, n_dimensions, requires_grad=True, device=device)\n",
        "optim_thetas = LBFGS([thetas], lr=0.1, max_iter=20, history_size=10, line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "# <<< CHANGE: Define the closure for the theta optimization\n",
        "def closure_thetas():\n",
        "    optim_thetas.zero_grad()\n",
        "    \n",
        "    thetas_constrained = torch.clamp(thetas, min=0) \n",
        "    a_params_constrained = torch.clamp(a_params_calibrated * Q_matrix, min=0)\n",
        "    \n",
        "    logits = torch.matmul(thetas_constrained, a_params_constrained.T) - ds_calibrated[None, :]\n",
        "    probs = torch.sigmoid(logits)\n",
        "    \n",
        "    log_likelihoods = Bernoulli(probs=probs).log_prob(data_with0) * train_idtor\n",
        "    loss = -log_likelihoods.sum() / train_idtor.sum()\n",
        "    \n",
        "    loss.backward()\n",
        "    return loss\n",
        "\n",
        "# <<< CHANGE: A single call to step() is needed, as max_iter is handled internally\n",
        "optim_thetas.step(closure_thetas)\n",
        "\n",
        "# Detach final parameters\n",
        "thetas_final = thetas.detach()\n",
        "a_params_final = a_params_calibrated\n",
        "ds_final = ds_calibrated\n",
        "print(\"Stage 2 Finished. Training complete.\")\n",
        "\n",
        "# --- 4. EVALUATION ---\n",
        "# Apply the final constraints before calculating metrics\n",
        "thetas_constrained_final = torch.clamp(thetas_final, min=0)\n",
        "a_params_constrained_final = torch.clamp(a_params_final * Q_matrix, min=0)\n",
        "\n",
        "# Use the correctly constrained parameters for prediction\n",
        "logits = torch.matmul(thetas_constrained_final, a_params_constrained_final.T) - ds_final[None, :]\n",
        "probs = torch.sigmoid(logits)\n",
        "\n",
        "train_auc, test_auc = compute_auc(probs, data_with0, train_idtor, test_idtor)\n",
        "train_cttcorr, test_cttcorr = compute_cttcorr(probs, data_with0, train_idtor, test_idtor)\n",
        "\n",
        "# --- 5. POST-HOC ANALYSIS ---\n",
        "print(\"\\n--- Post-Hoc Analysis ---\")\n",
        "\n",
        "# Ensure the final thetas are on the CPU and converted to a NumPy array\n",
        "thetas_np = thetas_final.cpu().numpy()\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "# rowvar=False is important: it tells numpy that your variables are columns, not rows.\n",
        "theta_correlation_matrix = np.corrcoef(thetas_np, rowvar=False)\n",
        "\n",
        "print(\"Theta Correlation Matrix:\")\n",
        "print(theta_correlation_matrix)\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Values close to 0 mean the dimensions are distinct.\")\n",
        "print(\"- Values close to 1 or -1 mean the dimensions are highly related.\")\n",
        "import torch\n",
        "\n",
        "# --- 6. MODEL FIT CALCULATION ---\n",
        "print(\"\\n--- Model Fit Indices ---\")\n",
        "\n",
        "# 1. Define Model Complexity (k = number of parameters)\n",
        "num_item_params = n_items * (n_dimensions + 1)  # (a * K) + d\n",
        "num_person_params = n_test_takers * n_dimensions # theta * K\n",
        "k = num_item_params + num_person_params\n",
        "print(f\"Number of parameters (k): {k}\")\n",
        "\n",
        "# 2. Define Number of Observations (n = number of responses in training set)\n",
        "n = train_idtor.sum().item()\n",
        "print(f\"Number of observations (n): {n}\")\n",
        "\n",
        "# 3. Calculate Final Log-Likelihood on the Training Data\n",
        "with torch.no_grad(): # We don't need to compute gradients here\n",
        "    # Use the final, constrained parameters\n",
        "    thetas_constrained_final = torch.clamp(thetas_final, min=0)\n",
        "    a_params_constrained_final = torch.clamp(a_params_final * Q_matrix, min=0)\n",
        "    \n",
        "    # Calculate probabilities for the training data\n",
        "    logits = torch.matmul(thetas_constrained_final, a_params_constrained_final.T) - ds_final[None, :]\n",
        "    probs = torch.sigmoid(logits)\n",
        "    \n",
        "    # Calculate the log-likelihood only on the training responses\n",
        "    log_likelihood = Bernoulli(probs=probs).log_prob(data_with0) * train_idtor\n",
        "    total_log_likelihood = log_likelihood.sum()\n",
        "\n",
        "# 4. Calculate AIC and BIC\n",
        "aic = 2 * k - 2 * total_log_likelihood\n",
        "bic = k * torch.log(torch.tensor(n, dtype=torch.float32)) - 2 * total_log_likelihood\n",
        "\n",
        "print(f\"\\nTotal Log-Likelihood: {total_log_likelihood.item():.2f}\")\n",
        "print(f\"AIC: {aic.item():.2f}\")\n",
        "print(f\"BIC: {bic.item():.2f}\")\n",
        "print(\"\\nReminder: Lower AIC/BIC values indicate a better model fit.\")\n",
        "# Clean up memory\n",
        "del thetas, a_params, ds, a_params_calibrated, ds_calibrated, thetas_final, a_params_final, ds_final, train_auc, test_auc, train_cttcorr, test_cttcorr\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "reeval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
