{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e8cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "from torch.distributions import Bernoulli\n",
    "from torch.optim import LBFGS\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import Manager\n",
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from tueplots import bundles\n",
    "bundles.icml2024()\n",
    "\n",
    "from torchmetrics import AUROC\n",
    "auroc = AUROC(task=\"binary\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "def visualize_response_matrix(results, value, filename):\n",
    "    # Extract the groups labels in the order of the columns\n",
    "    group_values = results.columns.get_level_values(\"scenario\")\n",
    "\n",
    "    # Identify the boundaries where the group changes\n",
    "    boundaries = []\n",
    "    for i in range(1, len(group_values)):\n",
    "        if group_values[i] != group_values[i - 1]:\n",
    "            boundaries.append(i - 0.5)  # using 0.5 to place the line between columns\n",
    "\n",
    "    # Visualize the results with a matrix: red is 0, white is -1 and blue is 1\n",
    "    cmap = mcolors.ListedColormap([\"white\", \"red\", \"blue\"])\n",
    "    bounds = [-1.5, -0.5, 0.5, 1.5]\n",
    "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    # Calculate midpoints for each group label\n",
    "    groups_list = list(group_values)\n",
    "    group_names = []\n",
    "    group_midpoints = []\n",
    "    current_group = groups_list[0]\n",
    "    start_index = 0\n",
    "    for i, grp in enumerate(groups_list):\n",
    "        if grp != current_group:\n",
    "            midpoint = (start_index + i - 1) / 2.0\n",
    "            group_names.append(current_group)\n",
    "            group_midpoints.append(midpoint)\n",
    "            current_group = grp\n",
    "            start_index = i\n",
    "    # Add the last group\n",
    "    midpoint = (start_index + len(groups_list) - 1) / 2.0\n",
    "    group_names.append(current_group)\n",
    "    group_midpoints.append(midpoint)\n",
    "\n",
    "    # Define the minimum spacing between labels (e.g., 100 units)\n",
    "    min_spacing = 100\n",
    "    last_label_pos = -float(\"inf\")\n",
    "    # Plot the matrix\n",
    "    with plt.rc_context(bundles.icml2024(usetex=True, family=\"serif\")):\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        cax = ax.matshow(value, aspect=\"auto\", cmap=cmap, norm=norm)\n",
    "\n",
    "        # Add vertical lines at each boundary\n",
    "        for b in boundaries:\n",
    "            ax.axvline(x=b, color=\"black\", linewidth=0.25, linestyle=\"--\", alpha=0.5)\n",
    "        \n",
    "        # Add group labels above the matrix, only if they're spaced enough apart\n",
    "        for name, pos in zip(group_names, group_midpoints):\n",
    "            if pos - last_label_pos >= min_spacing:\n",
    "                ax.text(pos, -5, name, ha='center', va='bottom', rotation=90, fontsize=3)\n",
    "                last_label_pos = pos\n",
    "\n",
    "        # Add model labels on the y-axis\n",
    "        ax.set_yticks(range(len(results.index)))\n",
    "        ax.set_yticklabels(results.index, fontsize=3)\n",
    "\n",
    "        # Add a colorbar\n",
    "        cbar = plt.colorbar(cax)\n",
    "        cbar.set_ticks([-1, 0, 1])\n",
    "        cbar.set_ticklabels([\"-1\", \"0\", \"1\"])\n",
    "        plt.savefig(filename, dpi=600, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "def trainer(parameters, optim, closure, n_iter=100, verbose=True):\n",
    "    pbar = tqdm(range(n_iter)) if verbose else range(n_iter)\n",
    "    for iteration in pbar:\n",
    "        if iteration > 0:\n",
    "            previous_parameters = [p.clone() for p in parameters]\n",
    "            previous_loss = loss.clone()\n",
    "        \n",
    "        loss = optim.step(closure)\n",
    "        \n",
    "        if iteration > 0:\n",
    "            d_loss = (previous_loss - loss).item()\n",
    "            d_parameters = sum(\n",
    "                torch.norm(prev - curr, p=2).item()\n",
    "                for prev, curr in zip(previous_parameters, parameters)\n",
    "            )\n",
    "            grad_norm = sum(torch.norm(p.grad, p=2).item() for p in parameters if p.grad is not None)\n",
    "            if verbose:\n",
    "                pbar.set_postfix({\"grad_norm\": grad_norm, \"d_parameter\": d_parameters, \"d_loss\": d_loss})\n",
    "            \n",
    "            if d_loss < 1e-5 and d_parameters < 1e-5 and grad_norm < 1e-5:\n",
    "                break\n",
    "    return parameters\n",
    "\n",
    "def compute_auc(probs, data, train_idtor, test_idtor):\n",
    "    train_probs = probs[train_idtor.bool()]\n",
    "    test_probs = probs[test_idtor.bool()]\n",
    "    train_labels = data[train_idtor.bool()]\n",
    "    test_labels = data[test_idtor.bool()]\n",
    "    train_auc = auroc(train_probs, train_labels)\n",
    "    test_auc = auroc(test_probs, test_labels)\n",
    "    print(f\"train auc: {train_auc}\")\n",
    "    print(f\"test auc: {test_auc}\")\n",
    "    \n",
    "    return train_auc, test_auc\n",
    "\n",
    "def compute_cttcorr(probs, data, train_idtor, test_idtor):\n",
    "    train_probs  = probs.clone()\n",
    "    test_probs   = probs.clone()\n",
    "    train_labels = data.clone()\n",
    "    test_labels  = data.clone()\n",
    "\n",
    "    train_mask = ~train_idtor.bool()\n",
    "    train_probs[train_mask]  = float('nan')\n",
    "    train_labels[train_mask] = float('nan')\n",
    "\n",
    "    test_mask = ~test_idtor.bool()\n",
    "    test_probs[test_mask]   = float('nan')\n",
    "    test_labels[test_mask]  = float('nan')\n",
    "    \n",
    "    train_prob_ctt = torch.nanmean(train_probs, dim=1).detach().cpu().numpy()\n",
    "    train_label_ctt = torch.nanmean(train_labels, dim=1).detach().cpu().numpy()\n",
    "    train_mask = ~np.isnan(train_prob_ctt) & ~np.isnan(train_label_ctt)\n",
    "    train_cttcorr = pearsonr(train_prob_ctt[train_mask], train_label_ctt[train_mask]).statistic\n",
    "    \n",
    "    test_prob_ctt = torch.nanmean(test_probs, dim=1).detach().cpu().numpy()\n",
    "    test_label_ctt = torch.nanmean(test_labels, dim=1).detach().cpu().numpy()\n",
    "    test_mask = ~np.isnan(test_prob_ctt) & ~np.isnan(test_label_ctt)\n",
    "    test_cttcorr = pearsonr(test_prob_ctt[test_mask], test_label_ctt[test_mask]).statistic\n",
    "    \n",
    "    print(f\"train cttcorr: {train_cttcorr}\")\n",
    "    print(f\"test cttcorr: {test_cttcorr}\")\n",
    "\n",
    "    return train_cttcorr, test_cttcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ddf5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading response matrix...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.special import expit # Numerically stable sigmoid function\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# --- PyTorch and Metrics Imports (from your reference code) ---\n",
    "import torch\n",
    "from torchmetrics import AUROC\n",
    "auroc = AUROC(task=\"binary\")\n",
    "# --- End of Imports ---\n",
    "# ===================================================================\n",
    "# == Step 1: Load Data and Create Train/Test Split\n",
    "# ===================================================================\n",
    "print(\"Loading response matrix...\")\n",
    "resmat = pd.read_pickle(\"../data/resmat_2000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c8d1940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into 910536 train samples and 227634 test samples.\n",
      "\n",
      "Training SVD model with k=8...\n",
      "\n",
      "Converting results to PyTorch Tensors for evaluation...\n",
      "\n",
      "--- Running Final Evaluations ---\n",
      "train auc: 0.9263309240341187\n",
      "test auc: 0.8913356065750122\n",
      "train cttcorr: 0.9637015461921692\n",
      "test cttcorr: 0.9530106782913208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(0.96370155), np.float32(0.9530107))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the locations (row, col indices) of all non-missing values\n",
    "non_nan_indices = np.argwhere(resmat.notna().values)\n",
    "\n",
    "# Randomly shuffle these indices\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(non_nan_indices)\n",
    "\n",
    "# Decide on the split size\n",
    "test_size = int(len(non_nan_indices) * 0.20)\n",
    "test_indices = non_nan_indices[:test_size]\n",
    "train_indices = non_nan_indices[test_size:]\n",
    "\n",
    "# Create the training matrix by hiding the test data\n",
    "train_resmat = resmat.copy()\n",
    "test_rows, test_cols = train_resmat.values.shape[0], train_resmat.values.shape[1]\n",
    "train_resmat.values[test_indices[:, 0], test_indices[:, 1]] = np.nan\n",
    "\n",
    "print(f\"Split data into {len(train_indices)} train samples and {len(test_indices)} test samples.\")\n",
    "\n",
    "# Impute the training data for SVD\n",
    "imputed_train_resmat = train_resmat.fillna(0)\n",
    "\n",
    "# ===================================================================\n",
    "# == Step 2: Train SVD Model and Get Predictions\n",
    "# ===================================================================\n",
    "OPTIMAL_K = 8\n",
    "print(f\"\\nTraining SVD model with k={OPTIMAL_K}...\")\n",
    "\n",
    "svd = TruncatedSVD(n_components=OPTIMAL_K, random_state=42)\n",
    "svd.fit(imputed_train_resmat)\n",
    "\n",
    "# Reconstruct the full matrix and convert to probabilities\n",
    "reconstructed_matrix = svd.inverse_transform(svd.transform(imputed_train_resmat))\n",
    "probs_matrix_np = expit(reconstructed_matrix)\n",
    "\n",
    "# ===================================================================\n",
    "# == Step 3: Prepare Data for Evaluation (NumPy and PyTorch)\n",
    "# ===================================================================\n",
    "# Ground truth data as a numpy array\n",
    "data_np = resmat.fillna(0).values\n",
    "\n",
    "# Create boolean masks (numpy)\n",
    "train_idtor_np = np.zeros_like(data_np, dtype=bool)\n",
    "test_idtor_np = np.zeros_like(data_np, dtype=bool)\n",
    "train_idtor_np[train_indices[:, 0], train_indices[:, 1]] = True\n",
    "test_idtor_np[test_indices[:, 0], test_indices[:, 1]] = True\n",
    "\n",
    "# --- NEW: Convert NumPy arrays to PyTorch tensors for compatibility ---\n",
    "print(\"\\nConverting results to PyTorch Tensors for evaluation...\")\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "probs_tensor = torch.tensor(probs_matrix_np, dtype=torch.float32, device=device)\n",
    "data_tensor = torch.tensor(data_np, dtype=torch.float32, device=device)\n",
    "train_idtor_tensor = torch.tensor(train_idtor_np, dtype=torch.int, device=device)\n",
    "test_idtor_tensor = torch.tensor(test_idtor_np, dtype=torch.int, device=device)\n",
    "# --- End of Conversion ---\n",
    "\n",
    "print(\"\\n--- Running Final Evaluations ---\")\n",
    "compute_auc(probs_tensor, data_tensor, train_idtor_tensor, test_idtor_tensor)\n",
    "compute_cttcorr(probs_tensor, data_tensor, train_idtor_tensor, test_idtor_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269b4440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Creating a single cell-level train/test split ---\n",
      "  Trial 0: Valid split condition met: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.special import expit\n",
    "from scipy.stats import pearsonr\n",
    "from torchmetrics import AUROC\n",
    "# ===================================================================\n",
    "# == Step 1: Create a Single, Unified Train/Test Split\n",
    "# ===================================================================\n",
    "print(\"--- Step 1: Creating a single cell-level train/test split ---\")\n",
    "resmat = pd.read_pickle(\"../data/resmat_2000.pkl\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_withnan = torch.tensor(resmat.values, dtype=torch.float32)\n",
    "data_idtor = torch.isfinite(data_withnan) # Mask of all known values\n",
    "\n",
    "valid_condition = False\n",
    "trial = 0\n",
    "while not valid_condition:\n",
    "    train_idtor = torch.bernoulli(data_idtor.float() * 0.8).int()\n",
    "    test_idtor = data_idtor.int() - train_idtor\n",
    "    # Ensure no row or column in the training set is entirely empty\n",
    "    valid_condition = (train_idtor.sum(dim=1) > 0).all() and (train_idtor.sum(dim=0) > 0).all()\n",
    "    print(f\"  Trial {trial}: Valid split condition met: {valid_condition}\")\n",
    "    trial += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf8a9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Training SVD on the training set ONLY ---\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# == Step 2: Train SVD *ONLY* on the Training Set\n",
    "# ===================================================================\n",
    "print(\"\\n--- Step 2: Training SVD on the training set ONLY ---\")\n",
    "# Create a training matrix by hiding the test values\n",
    "train_resmat_df = resmat.copy()\n",
    "train_resmat_df[test_idtor.bool().numpy()] = np.nan\n",
    "imputed_train_resmat = train_resmat_df.fillna(0)\n",
    "\n",
    "OPTIMAL_K = 8\n",
    "svd_model = TruncatedSVD(n_components=OPTIMAL_K, random_state=42)\n",
    "# Fit the SVD model ONLY on the training data\n",
    "svd_model.fit(imputed_train_resmat)\n",
    "\n",
    "# Get the item parameters (y target for NN) from this training-only model\n",
    "svd_item_params = svd_model.components_\n",
    "# Get the model abilities by transforming the FULL imputed data\n",
    "svd_model_abilities = svd_model.transform(resmat.fillna(0).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6d5ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Training Neural Network to predict item parameters ---\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# == Step 3: Train NN to Predict Item Parameters\n",
    "# ===================================================================\n",
    "print(\"\\n--- Step 3: Training Neural Network to predict item parameters ---\")\n",
    "\n",
    "with open(\"../data/embed_meta-llama_Llama-3.1-8B-Instruct.pkl\", \"rb\") as f:\n",
    "    df_embed = pickle.load(f)\n",
    "\n",
    "question_to_emb = dict(zip(df_embed[\"question\"], df_embed[\"embedding\"])); questions = resmat.columns.get_level_values(\"input.text\").tolist()\n",
    "embeds = [question_to_emb.get(q, None) for q in questions]\n",
    "# The target abilities are from the SVD trained ONLY on the training set\n",
    "y_abilities_all = svd_item_params.T\n",
    "\n",
    "combined_data = pd.DataFrame({'embedding': embeds, 'ability_vector': list(y_abilities_all)})\n",
    "cleaned_data = combined_data.dropna(subset=['embedding'])\n",
    "X_embeddings = np.vstack(cleaned_data['embedding'].values)\n",
    "y_abilities = np.vstack(cleaned_data['ability_vector'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "690fd022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Average Training Loss: 0.000435\n",
      "Epoch 2/200 | Average Training Loss: 0.000160\n",
      "Epoch 3/200 | Average Training Loss: 0.000110\n",
      "Epoch 4/200 | Average Training Loss: 0.000083\n",
      "Epoch 5/200 | Average Training Loss: 0.000067\n",
      "Epoch 6/200 | Average Training Loss: 0.000056\n",
      "Epoch 7/200 | Average Training Loss: 0.000049\n",
      "Epoch 8/200 | Average Training Loss: 0.000044\n",
      "Epoch 9/200 | Average Training Loss: 0.000039\n",
      "Epoch 10/200 | Average Training Loss: 0.000037\n",
      "Epoch 11/200 | Average Training Loss: 0.000034\n",
      "Epoch 12/200 | Average Training Loss: 0.000033\n",
      "Epoch 13/200 | Average Training Loss: 0.000032\n",
      "Epoch 14/200 | Average Training Loss: 0.000031\n",
      "Epoch 15/200 | Average Training Loss: 0.000030\n",
      "Epoch 16/200 | Average Training Loss: 0.000030\n",
      "Epoch 17/200 | Average Training Loss: 0.000029\n",
      "Epoch 18/200 | Average Training Loss: 0.000029\n",
      "Epoch 19/200 | Average Training Loss: 0.000028\n",
      "Epoch 20/200 | Average Training Loss: 0.000028\n",
      "Epoch 21/200 | Average Training Loss: 0.000027\n",
      "Epoch 22/200 | Average Training Loss: 0.000027\n",
      "Epoch 23/200 | Average Training Loss: 0.000026\n",
      "Epoch 24/200 | Average Training Loss: 0.000026\n",
      "Epoch 25/200 | Average Training Loss: 0.000026\n",
      "Epoch 26/200 | Average Training Loss: 0.000025\n",
      "Epoch 27/200 | Average Training Loss: 0.000025\n",
      "Epoch 28/200 | Average Training Loss: 0.000025\n",
      "Epoch 29/200 | Average Training Loss: 0.000025\n",
      "Epoch 30/200 | Average Training Loss: 0.000024\n",
      "Epoch 31/200 | Average Training Loss: 0.000024\n",
      "Epoch 32/200 | Average Training Loss: 0.000024\n",
      "Epoch 33/200 | Average Training Loss: 0.000024\n",
      "Epoch 34/200 | Average Training Loss: 0.000024\n",
      "Epoch 35/200 | Average Training Loss: 0.000023\n",
      "Epoch 36/200 | Average Training Loss: 0.000023\n",
      "Epoch 37/200 | Average Training Loss: 0.000023\n",
      "Epoch 38/200 | Average Training Loss: 0.000023\n",
      "Epoch 39/200 | Average Training Loss: 0.000023\n",
      "Epoch 40/200 | Average Training Loss: 0.000022\n",
      "Epoch 41/200 | Average Training Loss: 0.000022\n",
      "Epoch 42/200 | Average Training Loss: 0.000022\n",
      "Epoch 43/200 | Average Training Loss: 0.000022\n",
      "Epoch 44/200 | Average Training Loss: 0.000022\n",
      "Epoch 45/200 | Average Training Loss: 0.000022\n",
      "Epoch 46/200 | Average Training Loss: 0.000022\n",
      "Epoch 47/200 | Average Training Loss: 0.000022\n",
      "Epoch 48/200 | Average Training Loss: 0.000021\n",
      "Epoch 49/200 | Average Training Loss: 0.000021\n",
      "Epoch 50/200 | Average Training Loss: 0.000021\n",
      "Epoch 51/200 | Average Training Loss: 0.000021\n",
      "Epoch 52/200 | Average Training Loss: 0.000021\n",
      "Epoch 53/200 | Average Training Loss: 0.000021\n",
      "Epoch 54/200 | Average Training Loss: 0.000021\n",
      "Epoch 55/200 | Average Training Loss: 0.000021\n",
      "Epoch 56/200 | Average Training Loss: 0.000021\n",
      "Epoch 57/200 | Average Training Loss: 0.000021\n",
      "Epoch 58/200 | Average Training Loss: 0.000020\n",
      "Epoch 59/200 | Average Training Loss: 0.000020\n",
      "Epoch 60/200 | Average Training Loss: 0.000020\n",
      "Epoch 61/200 | Average Training Loss: 0.000020\n",
      "Epoch 62/200 | Average Training Loss: 0.000020\n",
      "Epoch 63/200 | Average Training Loss: 0.000020\n",
      "Epoch 64/200 | Average Training Loss: 0.000020\n",
      "Epoch 65/200 | Average Training Loss: 0.000020\n",
      "Epoch 66/200 | Average Training Loss: 0.000020\n",
      "Epoch 67/200 | Average Training Loss: 0.000020\n",
      "Epoch 68/200 | Average Training Loss: 0.000020\n",
      "Epoch 69/200 | Average Training Loss: 0.000020\n",
      "Epoch 70/200 | Average Training Loss: 0.000020\n",
      "Epoch 71/200 | Average Training Loss: 0.000020\n",
      "Epoch 72/200 | Average Training Loss: 0.000020\n",
      "Epoch 73/200 | Average Training Loss: 0.000019\n",
      "Epoch 74/200 | Average Training Loss: 0.000019\n",
      "Epoch 75/200 | Average Training Loss: 0.000019\n",
      "Epoch 76/200 | Average Training Loss: 0.000019\n",
      "Epoch 77/200 | Average Training Loss: 0.000019\n",
      "Epoch 78/200 | Average Training Loss: 0.000019\n",
      "Epoch 79/200 | Average Training Loss: 0.000019\n",
      "Epoch 80/200 | Average Training Loss: 0.000019\n",
      "Epoch 81/200 | Average Training Loss: 0.000019\n",
      "Epoch 82/200 | Average Training Loss: 0.000019\n",
      "Epoch 83/200 | Average Training Loss: 0.000019\n",
      "Epoch 84/200 | Average Training Loss: 0.000019\n",
      "Epoch 85/200 | Average Training Loss: 0.000019\n",
      "Epoch 86/200 | Average Training Loss: 0.000019\n",
      "Epoch 87/200 | Average Training Loss: 0.000019\n",
      "Epoch 88/200 | Average Training Loss: 0.000019\n",
      "Epoch 89/200 | Average Training Loss: 0.000019\n",
      "Epoch 90/200 | Average Training Loss: 0.000019\n",
      "Epoch 91/200 | Average Training Loss: 0.000019\n",
      "Epoch 92/200 | Average Training Loss: 0.000018\n",
      "Epoch 93/200 | Average Training Loss: 0.000018\n",
      "Epoch 94/200 | Average Training Loss: 0.000019\n",
      "Epoch 95/200 | Average Training Loss: 0.000019\n",
      "Epoch 96/200 | Average Training Loss: 0.000019\n",
      "Epoch 97/200 | Average Training Loss: 0.000019\n",
      "Epoch 98/200 | Average Training Loss: 0.000018\n",
      "Epoch 99/200 | Average Training Loss: 0.000018\n",
      "Epoch 100/200 | Average Training Loss: 0.000018\n",
      "Epoch 101/200 | Average Training Loss: 0.000018\n",
      "Epoch 102/200 | Average Training Loss: 0.000018\n",
      "Epoch 103/200 | Average Training Loss: 0.000018\n",
      "Epoch 104/200 | Average Training Loss: 0.000018\n",
      "Epoch 105/200 | Average Training Loss: 0.000018\n",
      "Epoch 106/200 | Average Training Loss: 0.000018\n",
      "Epoch 107/200 | Average Training Loss: 0.000018\n",
      "Epoch 108/200 | Average Training Loss: 0.000018\n",
      "Epoch 109/200 | Average Training Loss: 0.000018\n",
      "Epoch 110/200 | Average Training Loss: 0.000018\n",
      "Epoch 111/200 | Average Training Loss: 0.000018\n",
      "Epoch 112/200 | Average Training Loss: 0.000018\n",
      "Epoch 113/200 | Average Training Loss: 0.000018\n",
      "Epoch 114/200 | Average Training Loss: 0.000018\n",
      "Epoch 115/200 | Average Training Loss: 0.000018\n",
      "Epoch 116/200 | Average Training Loss: 0.000018\n",
      "Epoch 117/200 | Average Training Loss: 0.000018\n",
      "Epoch 118/200 | Average Training Loss: 0.000018\n",
      "Epoch 119/200 | Average Training Loss: 0.000018\n",
      "Epoch 120/200 | Average Training Loss: 0.000018\n",
      "Epoch 121/200 | Average Training Loss: 0.000018\n",
      "Epoch 122/200 | Average Training Loss: 0.000017\n",
      "Epoch 123/200 | Average Training Loss: 0.000018\n",
      "Epoch 124/200 | Average Training Loss: 0.000018\n",
      "Epoch 125/200 | Average Training Loss: 0.000018\n",
      "Epoch 126/200 | Average Training Loss: 0.000018\n",
      "Epoch 127/200 | Average Training Loss: 0.000017\n",
      "Epoch 128/200 | Average Training Loss: 0.000017\n",
      "Epoch 129/200 | Average Training Loss: 0.000017\n",
      "Epoch 130/200 | Average Training Loss: 0.000017\n",
      "Epoch 131/200 | Average Training Loss: 0.000017\n",
      "Epoch 132/200 | Average Training Loss: 0.000017\n",
      "Epoch 133/200 | Average Training Loss: 0.000017\n",
      "Epoch 134/200 | Average Training Loss: 0.000017\n",
      "Epoch 135/200 | Average Training Loss: 0.000017\n",
      "Epoch 136/200 | Average Training Loss: 0.000017\n",
      "Epoch 137/200 | Average Training Loss: 0.000017\n",
      "Epoch 138/200 | Average Training Loss: 0.000017\n",
      "Epoch 139/200 | Average Training Loss: 0.000017\n",
      "Epoch 140/200 | Average Training Loss: 0.000017\n",
      "Epoch 141/200 | Average Training Loss: 0.000017\n",
      "Epoch 142/200 | Average Training Loss: 0.000017\n",
      "Epoch 143/200 | Average Training Loss: 0.000017\n",
      "Epoch 144/200 | Average Training Loss: 0.000017\n",
      "Epoch 145/200 | Average Training Loss: 0.000017\n",
      "Epoch 146/200 | Average Training Loss: 0.000017\n",
      "Epoch 147/200 | Average Training Loss: 0.000017\n",
      "Epoch 148/200 | Average Training Loss: 0.000017\n",
      "Epoch 149/200 | Average Training Loss: 0.000017\n",
      "Epoch 150/200 | Average Training Loss: 0.000017\n",
      "Epoch 151/200 | Average Training Loss: 0.000017\n",
      "Epoch 152/200 | Average Training Loss: 0.000017\n",
      "Epoch 153/200 | Average Training Loss: 0.000017\n",
      "Epoch 154/200 | Average Training Loss: 0.000017\n",
      "Epoch 155/200 | Average Training Loss: 0.000017\n",
      "Epoch 156/200 | Average Training Loss: 0.000017\n",
      "Epoch 157/200 | Average Training Loss: 0.000017\n",
      "Epoch 158/200 | Average Training Loss: 0.000016\n",
      "Epoch 159/200 | Average Training Loss: 0.000017\n",
      "Epoch 160/200 | Average Training Loss: 0.000017\n",
      "Epoch 161/200 | Average Training Loss: 0.000017\n",
      "Epoch 162/200 | Average Training Loss: 0.000017\n",
      "Epoch 163/200 | Average Training Loss: 0.000016\n",
      "Epoch 164/200 | Average Training Loss: 0.000016\n",
      "Epoch 165/200 | Average Training Loss: 0.000016\n",
      "Epoch 166/200 | Average Training Loss: 0.000016\n",
      "Epoch 167/200 | Average Training Loss: 0.000016\n",
      "Epoch 168/200 | Average Training Loss: 0.000016\n",
      "Epoch 169/200 | Average Training Loss: 0.000016\n",
      "Epoch 170/200 | Average Training Loss: 0.000016\n",
      "Epoch 171/200 | Average Training Loss: 0.000016\n",
      "Epoch 172/200 | Average Training Loss: 0.000016\n",
      "Epoch 173/200 | Average Training Loss: 0.000016\n",
      "Epoch 174/200 | Average Training Loss: 0.000016\n",
      "Epoch 175/200 | Average Training Loss: 0.000017\n",
      "Epoch 176/200 | Average Training Loss: 0.000016\n",
      "Epoch 177/200 | Average Training Loss: 0.000016\n",
      "Epoch 178/200 | Average Training Loss: 0.000016\n",
      "Epoch 179/200 | Average Training Loss: 0.000016\n",
      "Epoch 180/200 | Average Training Loss: 0.000016\n",
      "Epoch 181/200 | Average Training Loss: 0.000016\n",
      "Epoch 182/200 | Average Training Loss: 0.000016\n",
      "Epoch 183/200 | Average Training Loss: 0.000016\n",
      "Epoch 184/200 | Average Training Loss: 0.000016\n",
      "Epoch 185/200 | Average Training Loss: 0.000016\n",
      "Epoch 186/200 | Average Training Loss: 0.000016\n",
      "Epoch 187/200 | Average Training Loss: 0.000016\n",
      "Epoch 188/200 | Average Training Loss: 0.000016\n",
      "Epoch 189/200 | Average Training Loss: 0.000016\n",
      "Epoch 190/200 | Average Training Loss: 0.000016\n",
      "Epoch 191/200 | Average Training Loss: 0.000016\n",
      "Epoch 192/200 | Average Training Loss: 0.000016\n",
      "Epoch 193/200 | Average Training Loss: 0.000016\n",
      "Epoch 194/200 | Average Training Loss: 0.000016\n",
      "Epoch 195/200 | Average Training Loss: 0.000016\n",
      "Epoch 196/200 | Average Training Loss: 0.000016\n",
      "Epoch 197/200 | Average Training Loss: 0.000016\n",
      "Epoch 198/200 | Average Training Loss: 0.000016\n",
      "Epoch 199/200 | Average Training Loss: 0.000016\n",
      "Epoch 200/200 | Average Training Loss: 0.000015\n",
      "--- NN Training Complete ---\n",
      "\n",
      "--- Step 4: Reconstructing matrix and running final evaluation ---\n",
      "\n",
      "--- Final Evaluation of NN-based Reconstruction ---\n",
      "train auc: 0.8902633190155029\n",
      "test auc: 0.8801394104957581\n",
      "train cttcorr: 0.9469870328903198\n",
      "test cttcorr: 0.9429172277450562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(0.94698703), np.float32(0.9429172))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AbilityPredictor(nn.Module):\n",
    "    def __init__(self, e, n): \n",
    "        super(AbilityPredictor, self).__init__();\n",
    "        self.network = nn.Sequential(nn.Linear(e, 1024),\n",
    "                                     nn.ReLU(), nn.Dropout(0.5),\n",
    "                                     nn.Linear(1024, 512),\n",
    "                                     nn.ReLU(), nn.Dropout(0.5),\n",
    "                                     nn.Linear(512, 128),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Dropout(0.5),\n",
    "                                     nn.Linear(128, n))\n",
    "    def forward(self, x): return self.network(x)\n",
    "\n",
    "# Note: We train the NN on ALL available items, because the target (y_abilities)\n",
    "# was already derived purely from the training set, so there is no data leakage.\n",
    "X_train_t = torch.tensor(X_embeddings, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_abilities, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t); train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "nn_model = AbilityPredictor(X_embeddings.shape[1], y_abilities.shape[1]); nn_model.to(device)\n",
    "loss_fn = nn.MSELoss(); optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.0001)\n",
    "\n",
    "n_epochs = 200\n",
    "for epoch in range(n_epochs):\n",
    "    nn_model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        p = nn_model(X_batch); loss = loss_fn(p, y_batch)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step(); total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Average Training Loss: {avg_loss:.6f}\")\n",
    "print(\"--- NN Training Complete ---\")\n",
    "\n",
    "# ===================================================================\n",
    "# == Step 4: Reconstruct and Evaluate with the Unified Split\n",
    "# ===================================================================\n",
    "print(\"\\n--- Step 4: Reconstructing matrix and running final evaluation ---\")\n",
    "\n",
    "embed_dim = X_embeddings.shape[1]\n",
    "full_embeds_np = np.zeros((len(embeds), embed_dim))\n",
    "for i, emb in enumerate(embeds):\n",
    "    if emb is not None: full_embeds_np[i] = emb\n",
    "\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.tensor(full_embeds_np, dtype=torch.float32).to(device)\n",
    "    nn_predicted_item_params = nn_model(input_tensor).cpu().numpy()\n",
    "\n",
    "# Reconstruct the logit matrix\n",
    "reconstructed_matrix_from_nn = svd_model_abilities @ nn_predicted_item_params.T\n",
    "# Convert logits to probabilities\n",
    "probs_matrix_from_nn = expit(reconstructed_matrix_from_nn)\n",
    "\n",
    "# Prepare all final tensors for evaluation\n",
    "probs_tensor = torch.tensor(probs_matrix_from_nn, dtype=torch.float32, device=device)\n",
    "data_tensor = torch.tensor(resmat.fillna(0).values, dtype=torch.float32, device=device)\n",
    "train_idtor_tensor = train_idtor.to(device)\n",
    "test_idtor_tensor = test_idtor.to(device)\n",
    "\n",
    "# Run the final evaluations using the unified train/test split\n",
    "print(\"\\n--- Final Evaluation of NN-based Reconstruction ---\")\n",
    "compute_auc(probs_tensor, data_tensor, train_idtor_tensor, test_idtor_tensor)\n",
    "compute_cttcorr(probs_tensor, data_tensor, train_idtor_tensor, test_idtor_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a461a07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final model saved to final_ability_predictor_model.pth ---\n"
     ]
    }
   ],
   "source": [
    "# After the training loop is finished\n",
    "torch.save(nn_model.state_dict(), '../result/final_ability_predictor_model.pth')\n",
    "print(\"\\n--- Final model saved to final_ability_predictor_model.pth ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
